---
date: 2019-06-04
title: "Using cosine similarity to find matching documents: a tutorial using Seneca's letters to his friend Lucilius"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/cosine_sim
    weight: 1
---

<div style="text-align:center;">
  <a href="https://en.wikipedia.org/wiki/Seneca_the_Younger">
    <img src="/img/seneca.png" title = "Seneca the Younger" width="400" height="600"></a>
</div>

Lately I've been interested in trying to cluster documents, and to find similar documents based on their contents.
In this blog post, I will use [Seneca's *Moral letters to Lucilius*](https://en.wikisource.org/wiki/Moral_letters_to_Lucilius)
and compute the pairwise [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of his 124 letters.
Computing the cosine similarity between two vectors returns how similar these vectors are. A cosine
similarity of 1 means that the angle between the two vectors is 0, and thus both vectors have the 
same direction.
Seneca's Moral letters to Lucilius deal mostly with philosophical topics, as Seneca was, among many other 
things, a philosopher of the stoic school. The stoic school of philosophy is quite
interesting, but it has been unfortunately misunderstood, especially in modern times. There is now a renewed interest for
this school, see [Modern Stoicism](https://en.wikipedia.org/wiki/Modern_Stoicism).

```{r, include = FALSE}
library(tidyverse)
```

The first step is to scrape the letters. The code below scrapes the letters, and saves them into a list.
I first start by writing a function that gets the raw text. Note the `xpath` argument of the `html_nodes()`
function. I obtained this complex expression by using the [SelectorGadget](https://selectorgadget.com/)
extension for Google Chrome, and then selecting the right element of the web page. 
See this [screenshot](https://i.imgur.com/2cntugt.png) if my description was not very clear.

Then, the `extract_text()` function extracts the text from the letter. The only line that might be 
a bit complex is ```discard(~`==`(., ""))``` which removes every empty line.

Finally, there's the `get_letter()` function that actually gets the letter by calling the first two 
functions. In the last line, I get all the letters into a list by mapping the list of urls to the 
`get_letter()` function.

```{r, cache = TRUE}
library(tidyverse)
library(rvest)

base_url <- "https://en.wikisource.org/wiki/Moral_letters_to_Lucilius/Letter_"

letter_numbers <- seq(1, 124)

letter_urls <- paste0(base_url, letter_numbers)

get_raw_text <- function(base_url, letter_number){
  paste0(base_url, letter_number) %>%
    read_html() %>%
    html_nodes(xpath ='//*[contains(concat( " ", @class, " " ), concat( " ", "mw-parser-output", " " ))]') %>%  
    html_text()
}


extract_text <- function(raw_text, letter_number){
  raw_text <- raw_text %>%
    str_split("\n") %>%  
    flatten_chr() %>%  
    discard(~`==`(., ""))

  start <- 5

  end <- str_which(raw_text, "Footnotes*")

  raw_text[start:(end-1)] %>%
    str_remove_all("\\[\\d{1,}\\]") %>%
    str_remove_all("\\[edit\\]")
}

get_letter <- function(base_url, letter_number){

  raw_text <- get_raw_text(base_url, letter_number)

  extract_text(raw_text, letter_number)
}

letters_to_lucilius <- map2(base_url, letter_numbers, get_letter)
```

Now that we have the letters saved in a list, we need to process the text a little bit. In order
to compute the cosine similarity between the letters, I need to somehow represent them as vectors.
There are several ways of doing this, and I am going to compute the tf-idf of each letter. The tf-idf
will give me a vector for each letter, with zero and non-zero values. Zero values represent words
that are common to all letters, and thus do not have any *predictive power*. Non-zero values are 
words that are not present in all letters, but maybe only a few. I expect that letters that 
discuss death for example, will have the word death in them, and letters that do not discuss death
will not have this word. The word death thus has what I call *predictive power*, in that it helps
us distinguish the letters discussing death from the other letters that do not discuss it. The same
reasoning can be applied for any topic.

So, to get the tf-idf of each letter, I first need to put them in a tidy dataset. I will use the 
`{tidytext}` package for this. First, I load the required packages, convert each letter to a 
dataframe of one column that contains the text, and save the letter's titles into another list:

```{r}
library(tidytext)
library(SnowballC)
library(stopwords)
library(text2vec)

letters_to_lucilius_df <- map(letters_to_lucilius, ~tibble("text" = .))

letter_titles <- letters_to_lucilius_df %>%
  map(~slice(., 1)) %>%
  map(pull)
```

Now, I add this title to each dataframe as a new column, called title:

```{r}
letters_to_lucilius_df <-  map2(.x = letters_to_lucilius_df, .y = letter_titles,
                                ~mutate(.x, title = .y)) %>%
  map(~slice(., -1))
```

I can now use `unnest_tokens()` to transform the datasets. Before, I had the whole text of the letter
in one column. After using `unnest_tokens()` I now have a dataset with one row per word. This will
make it easy to compute frequencies by letters, or what I am interested in, the tf-idf of each letter:

```{r}
tokenized_letters <- letters_to_lucilius_df %>%
  bind_rows() %>%
  group_by(title) %>%
  unnest_tokens(word, text)
```

I can now remove stopwords, using the data containing in the `{stopwords}` package:

```{r}
stopwords_en <- tibble("word" = stopwords("en", source  = "smart"))

tokenized_letters <- tokenized_letters %>%
  anti_join(stopwords_en) %>%
  filter(!str_detect(word, "\\d{1,}"))
```

Next step, wordstemming, meaning, going from "dogs" to "dog", or from "was" to "be". If you do not
do wordstemming, "dogs" and "dog" will be considered different words, even though they are not.
`wordStem()` is a function from `{SnowballC}`. 

```{r}
tokenized_letters <- tokenized_letters %>%
  mutate(word = wordStem(word, language = "en"))
```

Finally, I can compute the tf-idf of each letter and cast the data as a sparse matrix:

```{r}
tfidf_letters <- tokenized_letters %>%
  count(title, word, sort  = TRUE) %>%
  bind_tf_idf(word, title, n)

sparse_matrix <- tfidf_letters %>%
  cast_sparse(title, word, tf)
```

Let's take a look at the sparse matrix:

```{r}
sparse_matrix[1:10, 1:4]
```

We can consider each row of this matrix as the vector representing a letter, and thus compute the
cosine similarity between letters. For this, I am using the `sim2()` function from the `{text2vec}` 
package. I then create the `get_similar_letters()` function that returns similar letters for a 
given reference letter:

```{r}
similarities <- sim2(sparse_matrix, method = "cosine", norm = "l2") 

get_similar_letters <- function(similarities, reference_letter, n_recommendations = 3){
  sort(similarities[reference_letter, ], decreasing = TRUE)[1:(2 + n_recommendations)]
}
```

```{r}
get_similar_letters(similarities, 19)
get_similar_letters(similarities, 99)
get_similar_letters(similarities, 32)
get_similar_letters(similarities, 101)
```

As we can see from these examples, this seems to be working quite well: the first title is the 
title of the reference letter, will the next 3 are the suggested letters. The problem is that my 
matrix is not in the right order, and thus reference letter 19 does not correspond to letter 19
of Seneca... I have to correct that, but not today.

Hope you enjoyed! If you found this blog post useful, you might want to follow 
me on [twitter](https://www.twitter.com/brodriguesco) for blog post updates and 
[buy me an espresso](https://www.buymeacoffee.com/brodriguesco) or [paypal.me](https://www.paypal.me/brodriguesco).

<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}</style><link href="https://fonts.googleapis.com/css?family=Cookie" rel="stylesheet"><a class="bmc-button" target="_blank" href="https://www.buymeacoffee.com/brodriguesco"><img src="https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg" alt="Buy me an Espresso"><span style="margin-left:5px">Buy me an Espresso</span></a>
